#!/bin/bash

# Sync and import GitHub scraper content
# Runs: Pull latest from GitHub, then import new content

echo "ğŸ”„ Starting GitHub content sync..."
echo "Timestamp: $(date)"

# Change to gdelt-obsidian directory
cd /Users/davidlockie/Documents/Projects/gdelt-obsidian || exit 1

# Fetch and reset to latest (more reliable than pull for automation)
echo "ğŸ“¥ Fetching latest content from GitHub..."
# Use gh CLI for authentication (works in cron without user interaction)
GIT_ASKPASS=/opt/homebrew/bin/gh git fetch origin master

# Check if fetch was successful
if [ $? -eq 0 ]; then
    echo "âœ… Git fetch successful"

    # Reset to origin/master (discards any local changes)
    echo "ğŸ§¹ Resetting to origin/master..."
    git reset --hard origin/master
    git clean -fd

    echo "âœ… Sync complete"
else
    echo "âŒ Git fetch failed"
    exit 1
fi

# Change to Eliza directory
cd /Users/davidlockie/Documents/Projects/Eliza || exit 1

# Load nvm and run import script
echo "ğŸ“š Importing new documents..."
export NVM_DIR="$HOME/.nvm"
[ -s "$NVM_DIR/nvm.sh" ] && \. "$NVM_DIR/nvm.sh"
nvm use 23.3.0 > /dev/null 2>&1
node import-github-scrapers.js

# Check import result
if [ $? -eq 0 ]; then
    echo "âœ… Import complete"
    echo "ğŸ‰ GitHub content sync finished successfully"
else
    echo "âŒ Import failed"
    exit 1
fi
